{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please read the README txt file for dependency versions\n",
    "#Also this is trained on Indian dataset so it will have bias but this is fine due to our usecase targeted that\n",
    "#The whole workflow from Dataset creation to training is explained in the readme file so please read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e313528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1d5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36ccca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_13676\\1112888837.py:1: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b5dbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x):\n",
    "    layer_norm=tf.keras.layers.LayerNormalization()\n",
    "    x=layer_norm(x)\n",
    "    layer_bright=tf.keras.layers.RandomBrightness(factor=0.2)\n",
    "    layer_contrast=tf.keras.layers.RandomContrast(factor=0.2)\n",
    "    x=layer_bright(x)\n",
    "    x=layer_contrast(x)\n",
    "    layer_resize=tf.keras.layers.Resizing(height=224,width=224)\n",
    "    x=layer_resize(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fafe5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model=tf.keras.applications.ResNet50(\n",
    "      include_top=False\n",
    "  )\n",
    "base_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b143c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "#maybe\n",
    "import tensorflow as tf\n",
    "\n",
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "        self.avg_pool = tf.keras.layers.AveragePooling2D(pool_size=(7, 7),strides=(1, 1), padding='same')\n",
    "        self.max_pool = tf.keras.layers.MaxPooling2D(pool_size=(7, 7),strides=(1, 1), padding='same')\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(in_channels // reduction_ratio, activation='relu', use_bias=False)\n",
    "        self.fc2 = tf.keras.layers.Dense(in_channels, use_bias=False)\n",
    "\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        avg_out = self.avg_pool(inputs)\n",
    "        max_out = self.max_pool(inputs)\n",
    "\n",
    "        avg_out = self.fc2(self.fc1(avg_out))\n",
    "        max_out = self.fc2(self.fc1(max_out))\n",
    "\n",
    "        out = avg_out + max_out\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "channel_attent = ChannelAttention(in_channels=2048)\n",
    "input_tensor = tf.random.normal((1, 7, 7, 2048))  \n",
    "output_tensor = channel_attent(input_tensor)\n",
    "print(output_tensor.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98bb5ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SpatialAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=1, kernel_size=7, strides=1, padding='same', activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Average pooling across channels\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=3, keepdims=True)\n",
    "        max_pool = tf.reduce_max(inputs, axis=3, keepdims=True)\n",
    "        concatenated = tf.concat([avg_pool, max_pool], axis=3)\n",
    "      \n",
    "        attention_map = self.conv(concatenated)\n",
    "        return inputs * attention_map\n",
    "\n",
    "spatial_attent = SpatialAttention()\n",
    "input_tensor = tf.random.normal((1, 7, 7, 2048))  \n",
    "output_tensor = spatial_attent(input_tensor)\n",
    "print(output_tensor.shape)  # Output shape should be (1, 7, 7, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf49ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import numpy as np\n",
    "\n",
    "class CosFaceLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes, s=30.0, m=0.35):\n",
    "        super(CosFaceLayer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                 shape=(input_shape[-1], self.num_classes),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.linalg.l2_normalize(inputs, axis=1)\n",
    "\n",
    "        W = tf.linalg.l2_normalize(self.W, axis=0)\n",
    "\n",
    "        cos_theta = tf.matmul(x, W)\n",
    "\n",
    "        return cos_theta * self.s\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_classes)\n",
    "def cosface_loss(y_true, y_pred):\n",
    "    m=0.35\n",
    "    s=30.0\n",
    "    y_true = tf.cast(tf.reshape(y_true, [-1]), tf.int32)\n",
    "    cos_theta = tf.reshape(y_pred, [tf.shape(y_true)[0], -1])\n",
    "\n",
    "    num_classes = tf.shape(cos_theta)[1]\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "\n",
    "    indices = tf.stack([tf.range(batch_size), y_true], axis=1)\n",
    "    cos_theta_yi = tf.gather_nd(cos_theta, indices)\n",
    "\n",
    "\n",
    "    cos_theta_yi_m = cos_theta_yi - m\n",
    "\n",
    "    mask = tf.one_hot(y_true, depth=num_classes)\n",
    "\n",
    "\n",
    "    cos_theta_yi_m = tf.expand_dims(cos_theta_yi_m, 1)\n",
    "\n",
    "\n",
    "    updated_logits = cos_theta * (1.0 - mask) + cos_theta_yi_m * mask\n",
    "\n",
    "    scaled_logits = s * updated_logits\n",
    "\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=scaled_logits)\n",
    "\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int64)  \n",
    "    y_pred_argmax = tf.argmax(y_pred, axis=-1)\n",
    "    correct = tf.equal(y_true, y_pred_argmax)\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2446bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"identity_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 7, 7, 2048)]      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 1024)        18875392  \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 7, 7, 1024)        4096      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 512)         4719104   \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 7, 7, 512)         2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 1024)              4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " identity_features (Dense)   (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " cos_face_layer (CosFaceLay  (None, 276)               141312    \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24798208 (94.60 MB)\n",
      "Trainable params: 24792064 (94.57 MB)\n",
      "Non-trainable params: 6144 (24.00 KB)\n",
      "_________________________________________________________________\n",
      "Model: \"age_estimation_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " agefeaturesmap (InputLayer  [(None, 7, 7, 2048)]      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 7, 1024)        18875392  \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 7, 7, 1024)        4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 7, 512)         4719104   \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 7, 7, 512)         2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 512)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 1024)              4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " age_estimation_output (Den  (None, 101)               51813     \n",
      " se)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24708709 (94.26 MB)\n",
      "Trainable params: 24702565 (94.23 MB)\n",
      "Non-trainable params: 6144 (24.00 KB)\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_image (InputLayer)    [(None, 7, 7, 2048)]         0         []                            \n",
      "                                                                                                  \n",
      " agefeaturesmap (InputLayer  [(None, 7, 7, 2048)]         0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " identity_model (Functional  (None, 276)                  2479820   ['input_image[0][0]']         \n",
      " )                                                        8                                       \n",
      "                                                                                                  \n",
      " age_estimation_model (Func  (None, 101)                  2470870   ['agefeaturesmap[0][0]']      \n",
      " tional)                                                  9                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 49506917 (188.85 MB)\n",
      "Trainable params: 49494629 (188.81 MB)\n",
      "Non-trainable params: 12288 (48.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Flatten,Dropout,Reshape,Conv2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "\n",
    "input_image = Input(shape=(7, 7, 2048))\n",
    "x = Conv2D(1024, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(1e-4))(input_image)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Reshape((512,))(x)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "identity_features = Dense(512, activation='relu', name='identity_features')(x)\n",
    "identity_features = BatchNormalization()(identity_features)\n",
    "identity_features=CosFaceLayer(num_classes=276)(identity_features)\n",
    "identity_model = Model(inputs=input_image, outputs=identity_features,name='identity_model')\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "first_decay_steps = 1000\n",
    "lr_schedule = CosineDecayRestarts(\n",
    "    initial_learning_rate,\n",
    "    first_decay_steps,\n",
    "    t_mul=2.0,\n",
    "    m_mul=0.9,\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "identity_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0),\n",
    "                       loss=cosface_loss,\n",
    "                       metrics=[custom_accuracy])\n",
    "\n",
    "agefeaturesmap = Input(shape=(7, 7, 2048), name='agefeaturesmap')\n",
    "x = Conv2D(1024, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(1e-4))(agefeaturesmap)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Reshape((512,))(x)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(512, activation='relu',kernel_regularizer=l2(1e-4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "age_estimation_output = Dense(101, activation='softmax', name='age_estimation_output')(x)\n",
    "\n",
    "age_Estimation_model = Model(inputs=agefeaturesmap, outputs=age_estimation_output,name='age_estimation_model')\n",
    "age_Estimation_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0),\n",
    "                             loss='sparse_categorical_crossentropy',\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "identity_model.summary()\n",
    "age_Estimation_model.summary()\n",
    "\n",
    "#No use of the combined model architecture as we are only using the identity model from it\n",
    "combined_input_age = Input(shape=(7, 7, 2048), name='agefeaturesmap')\n",
    "combined_input_identity = Input(shape=(7,7,2048), name='input_image')\n",
    "\n",
    "identity_output = identity_model(combined_input_identity)\n",
    "age_output = age_Estimation_model(combined_input_age)\n",
    "combined_model = Model(inputs=[combined_input_age, combined_input_identity],\n",
    "                       outputs=[identity_output, age_output])\n",
    "combined_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0),\n",
    "                           loss={\n",
    "        'identity_model': cosface_loss,\n",
    "        'age_estimation_model': 'sparse_categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'identity_model': custom_accuracy,\n",
    "        'age_estimation_model': 'accuracy'\n",
    "    }\n",
    ")\n",
    "combined_model.build([(7, 7, 2048), (7, 7, 2048)])\n",
    "\n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d643535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37cb7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, lambda_param=1.0):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            return tf.reverse_gradient(x, self.lambda_param)\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe40ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing 2\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "img2=Image.open(r'/kaggle/input/ayeshatakiatesting/Screenshot 2024-07-08 193006.png')\n",
    "img2=img2.convert('RGB')\n",
    "imgarray2=np.array(img2)\n",
    "img1=Image.open(r'/kaggle/input/testing2/testsubject2.png')\n",
    "img1=img1.convert('RGB')\n",
    "imgarray1=np.array(img1)\n",
    "layer_norm=tf.keras.layers.LayerNormalization()\n",
    "layer_resize=tf.keras.layers.Resizing(height=224,width=224)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 10))\n",
    "ax1.imshow(img1)\n",
    "ax1.axis('off') \n",
    "ax1.set_title('Image 1')\n",
    "ax2.imshow(img2)\n",
    "ax2.axis('off') \n",
    "ax2.set_title('Image 2')\n",
    "ax1.set_aspect('equal')\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "preprocessed1=layer_resize(layer_norm(imgarray1))\n",
    "preprocessed2=layer_resize(layer_norm(imgarray2))\n",
    "\n",
    "prep1=tf.expand_dims(preprocessed1,axis=0)\n",
    "base1=base_model.predict(prep)\n",
    "\n",
    "chanoutput1=channel_attent(base1)\n",
    "spatoutput1=spatial_attent(base1)\n",
    "attentmask1=spatial_attent(channel_attent(chanoutput1+spatoutput1))\n",
    "grl = GradientReversalLayer()\n",
    "identityinput1=grl(attentmask1)*base1\n",
    "\n",
    "prep2=tf.expand_dims(preprocessed2,axis=0)\n",
    "base2=base_model.predict(prep2)\n",
    "\n",
    "chanoutput2=channel_attent(base2)\n",
    "spatoutput2=spatial_attent(base2)\n",
    "attentmask2=spatial_attent(channel_attent(chanoutput2+spatoutput2))\n",
    "grl = GradientReversalLayer()\n",
    "identityinput2=grl(attentmask2)*base2\n",
    "\n",
    "identity_model.load_weights(r'/kaggle/working/model_weights2/identity_model_weights25_epoch_10.weights.h5')\n",
    "identity_features_layer = identity_model.get_layer('identity_features')\n",
    "identity_features_extractor = Model(inputs=identity_model.input, \n",
    "                                    outputs=identity_features_layer.output,\n",
    "                                    name='identity_features_extractor')\n",
    "prediction1=(identity_features_extractor.predict(identityinput1))\n",
    "prediction2=(identity_features_extractor.predict(identityinput2))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a_normalized = tf.nn.l2_normalize(a, axis=-1)\n",
    "    b_normalized = tf.nn.l2_normalize(b, axis=-1)\n",
    "    \n",
    "    return tf.reduce_sum(tf.multiply(a_normalized, b_normalized), axis=-1)\n",
    "similarity=cosine_similarity(prediction1,prediction2)\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "html_string = f'''\n",
    "<div style=\"font-size:24px;\">\n",
    "    <p>Percent of Similarity: {similarity.numpy()*100}</p>\n",
    "</div>\n",
    "'''\n",
    "large_text = HTML(html_string)\n",
    "display(large_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE TRAINING IS DONE ON KAGGLE SO HERE IS THE LINK OF KAGGLE \n",
    "#https://www.kaggle.com/code/abhiekre/jinx-hopesohopeso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING ARC DOWN BELOW EVERY GENERATOR TARGETS DIFFERENT TENSORS IK ITS NOT THE BEST APPROACH BUT I DIDNT HAVE THAT MUCH RAM SO IF IT WORKS IT WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff96211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_Tensor5K=np.load(r'/kaggle/input/tensors/img_tensor5k.npy')\n",
    "label_tensor=np.load(r'/kaggle/input/tensors/labels.npy')\n",
    "def generator1():\n",
    "        for i in range(5000):\n",
    "            img = img_Tensor5K[i]\n",
    "            label = label_tensor[i]\n",
    "\n",
    "\n",
    "            inputt = tf.expand_dims(preprocessing(img), axis=0)\n",
    "  \n",
    "            feature_map = base_model(inputt, training=False)\n",
    "\n",
    "            chanoutput = channelattention(feature_map)\n",
    "            spatialoutput = spatialattention(feature_map)\n",
    "            attentionmask1 = chanoutput + spatialoutput\n",
    "            attentionmask2=channelattention(attentionmask1)\n",
    "            attentionmask=spatialattention(attentionmask2)\n",
    "\n",
    "            age_input = attentionmask * feature_map\n",
    "            grl = GradientReversalLayer()\n",
    "    \n",
    "            reversed_mask = grl(attentionmask)\n",
    "    \n",
    "            identity_input = tf.multiply(reversed_mask, feature_map)\n",
    "\n",
    "            identity = label[0]\n",
    "            age = label[1]\n",
    "\n",
    "            yield (identity_input, age_input), (identity, age)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator1,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(1,7, 7, 2048), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(1, 7, 7, 2048), dtype=tf.float32)),\n",
    "        (tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32))\n",
    "    )\n",
    ")\n",
    "\n",
    "batch_size = 20\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "channelattention = ChannelAttention(2048)\n",
    "spatialattention = SpatialAttention()\n",
    "\n",
    "dataset_size = 5000 //20  \n",
    "\n",
    "train_size = int(0.7 * dataset_size)  \n",
    "val_size = int(0.15 * dataset_size)  \n",
    "test_size = dataset_size - train_size - val_size  \n",
    "\n",
    "\n",
    "\n",
    "train_dataset5k = dataset.take(train_size)\n",
    "temp_dataset = dataset.skip(train_size)\n",
    "val_dataset5k = temp_dataset.take(val_size)\n",
    "test_dataset5k = temp_dataset.skip(val_size)\n",
    "\n",
    "train_identity_losses = []\n",
    "train_age_losses = []\n",
    "val_identity_losses = []\n",
    "val_age_losses = []\n",
    "\n",
    "save_dir=r'/kaggle/working/model_weights/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "train_dataset5k = train_dataset5k.shuffle(buffer_size=1000)\n",
    "identity_model.load_weights(r'/kaggle/working/model_weights/identity_model_weights8_epoch_61.weights.h5')\n",
    "age_Estimation_model.load_weights(r'/kaggle/working/model_weights/age_model_weights8_epoch_61.weights.h5')\n",
    "count=55\n",
    "for epoch in range(5):  \n",
    "    print(f\"Epoch {epoch + 1}/10\")\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(train_dataset5k):\n",
    "     \n",
    "        print(identity_inputs.shape)\n",
    "        print(age_inputs.shape)\n",
    "\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "        print(identity_inputs.shape)\n",
    "        print(age_inputs.shape)\n",
    "\n",
    "        history_identity = identity_model.fit(\n",
    "            x=identity_inputs, y=identities,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=20 \n",
    "        )\n",
    "\n",
    "        history_age = age_Estimation_model.fit(\n",
    "            x=age_inputs, y=ages,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=20  \n",
    "        )\n",
    "\n",
    "        print(f\"Batch {batch + 1} processed. \"\n",
    "              f\"Identity Loss: {history_identity.history['loss'][0]:.4f}, \"\n",
    "              f\"Age Loss: {history_age.history['loss'][0]:.4f}\")\n",
    "    val_identity_loss = []\n",
    "    val_age_loss = []\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(val_dataset5k):\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "\n",
    "        val_identity_pred = identity_model.predict(identity_inputs)\n",
    "\n",
    "        val_identity_loss.append(tf.keras.losses.sparse_categorical_crossentropy(identities, val_identity_pred))\n",
    "\n",
    "        ages = tf.cast(ages, tf.float32)\n",
    "\n",
    "        val_age_pred = age_Estimation_model.predict(age_inputs)\n",
    "\n",
    "        val_age_pred = tf.cast(tf.argmax(val_age_pred, axis=1), tf.float32)\n",
    "\n",
    "        val_age_loss.append(tf.reduce_mean(tf.abs(ages - val_age_pred)))\n",
    "        print('validation loss identity:',sum(val_identity_loss) / len(val_identity_loss))\n",
    "        print('validation loss age:',sum(val_identity_loss) / len(val_identity_loss))\n",
    "    val_identity_losses.append(sum(val_identity_loss) / len(val_identity_loss))\n",
    "    val_age_losses.append(sum(val_age_loss) / len(val_age_loss))\n",
    "    \n",
    "\n",
    "    if (epoch + 1) % 2 == 1:  \n",
    "        print(f\"Saving weights for epoch {epoch + 1}\")\n",
    "        identity_model.save_weights(os.path.join(save_dir,f'identity_model_weights14_epoch_{count+epoch + 1}.weights.h5'))\n",
    "        age_Estimation_model.save_weights(os.path.join(save_dir,f'age_model_weights14_epoch_{count+epoch + 1}.weights.h5'))\n",
    "\n",
    "    count+=1\n",
    "    tf.keras.backend.clear_session()\n",
    "epochs = range(1, len(train_identity_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_identity_losses, 'bo-', label='Training Identity Loss')\n",
    "plt.plot(epochs, val_identity_losses, 'ro-', label='Validation Identity Loss')\n",
    "plt.title('Identity Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_age_losses, 'bo-', label='Training Age Loss')\n",
    "plt.plot(epochs, val_age_losses, 'ro-', label='Validation Age Loss')\n",
    "plt.title('Age Estimation Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_Tensor10K=np.load(r'/kaggle/input/tensors/img_tensor10k.npy')\n",
    "\n",
    "label_tensor=np.load(r'/kaggle/input/tensors/labels.npy')\n",
    "def generator2():\n",
    "        gtrcount=5000\n",
    "        for i in range(0,5000):\n",
    "            img = img_Tensor10K[i]\n",
    "            label = label_tensor[gtrcount+i]\n",
    "\n",
    "\n",
    "\n",
    "            inputt = tf.expand_dims(preprocessing(img), axis=0)\n",
    "      \n",
    "            feature_map = base_model(inputt, training=False)\n",
    "\n",
    "            chanoutput = channelattention(feature_map)\n",
    "            spatialoutput = spatialattention(feature_map)\n",
    "            attentionmask1 = chanoutput + spatialoutput\n",
    "            attentionmask2=channelattention(attentionmask1)\n",
    "            attentionmask=spatialattention(attentionmask2)\n",
    "\n",
    "            age_input = attentionmask * feature_map\n",
    "            grl = GradientReversalLayer()\n",
    "    \n",
    "            reversed_mask = grl(attentionmask)\n",
    "            identity_input = tf.multiply(reversed_mask, feature_map)\n",
    "\n",
    "            identity = label[0]\n",
    "            age = label[1]\n",
    "            gtrcount+=1\n",
    "            yield (identity_input, age_input), (identity, age)\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_generator(\n",
    "    generator2,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(1,7, 7, 2048), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(1, 7, 7, 2048), dtype=tf.float32)),\n",
    "        (tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32))\n",
    "    )\n",
    ")\n",
    "\n",
    "batch_size = 30\n",
    "dataset2 = dataset2.batch(batch_size)\n",
    "\n",
    "dataset2 = dataset2.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "channelattention = ChannelAttention(2048)\n",
    "spatialattention = SpatialAttention()\n",
    "\n",
    "dataset_size = 5000 //30 \n",
    "\n",
    "train_size = int(0.7 * dataset_size)  \n",
    "val_size = int(0.15 * dataset_size)  \n",
    "test_size = dataset_size - train_size - val_size \n",
    "\n",
    "train_identity_losses = []\n",
    "train_age_losses = []\n",
    "val_identity_losses = []\n",
    "val_age_losses = []\n",
    "\n",
    "train_dataset10k = dataset2.take(train_size)\n",
    "temp_dataset = dataset2.skip(train_size)\n",
    "val_dataset10k = temp_dataset.take(val_size)\n",
    "test_datase10k = temp_dataset.skip(val_size)\n",
    "\n",
    "\n",
    "save_dir=r'/kaggle/working/model_weights'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "train_dataset10k = train_dataset10k.shuffle(buffer_size=1000)\n",
    "identity_model.load_weights(r'/kaggle/working/model_weights/identity_model_weights7_epoch_55.weights.h5',skip_mismatch=True)\n",
    "age_Estimation_model.load_weights(r'/kaggle/working/model_weights/age_model_weights7_epoch_55.weights.h5',skip_mismatch=True)\n",
    "count=56\n",
    "for epoch in range(10):  \n",
    "    print(f\"Epoch {epoch + 1}/10\")\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(train_dataset10k):\n",
    "       \n",
    "        print(identity_inputs.shape)\n",
    "        print(age_inputs.shape)\n",
    "\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "        print(identity_inputs.shape)\n",
    "        print(age_inputs.shape)\n",
    "   \n",
    "        history_identity = identity_model.fit(\n",
    "            x=identity_inputs, y=identities,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=30 \n",
    "        )\n",
    "\n",
    "        history_age = age_Estimation_model.fit(\n",
    "            x=age_inputs, y=ages,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=30 \n",
    "        )\n",
    "\n",
    "        print(f\"Batch {batch + 1} processed. \"\n",
    "              f\"Identity Loss: {history_identity.history['loss'][0]:.4f}, \"\n",
    "              f\"Age Loss: {history_age.history['loss'][0]:.4f}\")\n",
    "    val_identity_loss = []\n",
    "    val_age_loss = []\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(val_dataset10k):\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "\n",
    "        val_identity_pred = identity_model.predict(identity_inputs)\n",
    "\n",
    "        val_identity_loss.append(tf.keras.losses.sparse_categorical_crossentropy(identities, val_identity_pred))\n",
    "\n",
    "        ages = tf.cast(ages, tf.float32)\n",
    "\n",
    "        val_age_pred = age_Estimation_model.predict(age_inputs)\n",
    "\n",
    "        val_age_pred = tf.cast(tf.argmax(val_age_pred, axis=1), tf.float32)\n",
    "\n",
    "        val_age_loss.append(tf.reduce_mean(tf.abs(ages - val_age_pred)))\n",
    "        print(\"avg val identity loss:\",sum(val_identity_loss) / len(val_identity_loss))\n",
    "        print(\"avg val age loss\",sum(val_age_loss) / len(val_age_loss))\n",
    "    if(len(val_identity_loss)!=0 and len(val_identity_loss)!=0):\n",
    "        val_identity_losses.append(sum(val_identity_loss) / len(val_identity_loss))\n",
    "        val_age_losses.append(sum(val_age_loss) / len(val_age_loss))\n",
    "\n",
    "    if (epoch + 1) % 2 == 1:  \n",
    "        print(f\"Saving weights for epoch {epoch + 1}\")\n",
    "        identity_model.save_weights(os.path.join(save_dir,f'identity_model_weights8_epoch_{count+epoch + 1}.weights.h5'))\n",
    "        age_Estimation_model.save_weights(os.path.join(save_dir,f'age_model_weights8_epoch_{count+epoch + 1}.weights.h5'))\n",
    "    \n",
    "    count+=1\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "epochs = range(1, len(train_identity_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_identity_losses, 'bo-', label='Training Identity Loss')\n",
    "plt.plot(epochs, val_identity_losses, 'ro-', label='Validation Identity Loss')\n",
    "\n",
    "plt.title('Identity Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_age_losses, 'bo-', label='Training Age Loss')\n",
    "plt.plot(epochs, val_age_losses, 'ro-', label='Validation Age Loss')\n",
    "plt.title('Age Estimation Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_Tensor15K=np.load(r'/kaggle/input/tensors/img_tensor15k.npy')\n",
    "label_tensor=np.load(r'/kaggle/input/tensors/labels.npy')\n",
    "def generator3():\n",
    "        gtrcount=10000\n",
    "        for i in range(0,5000):\n",
    "            img = img_Tensor15K[i]\n",
    "            label = label_tensor[gtrcount+i]\n",
    "\n",
    "\n",
    "\n",
    "            inputt = tf.expand_dims(preprocessing(img), axis=0)\n",
    "\n",
    "            feature_map = base_model(inputt, training=False)\n",
    "\n",
    "            chanoutput = channelattention(feature_map)\n",
    "            spatialoutput = spatialattention(feature_map)\n",
    "            attentionmask1 = chanoutput + spatialoutput\n",
    "            attentionmask2=channelattention(attentionmask1)\n",
    "            attentionmask=spatialattention(attentionmask2)\n",
    "\n",
    "            age_input = attentionmask * feature_map\n",
    "            grl = GradientReversalLayer()\n",
    "    \n",
    "            reversed_mask = grl(attentionmask)\n",
    "    \n",
    "            identity_input = tf.multiply(reversed_mask, feature_map)\n",
    "\n",
    "            identity = label[0]\n",
    "            age = label[1]\n",
    "            gtrcount+=1\n",
    "            yield (identity_input, age_input), (identity, age)\n",
    "\n",
    "dataset3= tf.data.Dataset.from_generator(\n",
    "    generator3,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(1,7, 7, 2048), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(1, 7, 7, 2048), dtype=tf.float32)),\n",
    "        (tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32))\n",
    "    )\n",
    ")\n",
    "=\n",
    "batch_size = 30\n",
    "dataset3 = dataset3.batch(batch_size)\n",
    "\n",
    "dataset3 = dataset3.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "channelattention = ChannelAttention(2048)\n",
    "spatialattention = SpatialAttention()\n",
    "\n",
    "dataset_size = 5000 //30 \n",
    "\n",
    "train_size = int(0.7 * dataset_size) \n",
    "val_size = int(0.15 * dataset_size)  \n",
    "test_size = dataset_size - train_size - val_size  \n",
    "\n",
    "\n",
    "\n",
    "train_dataset15k = dataset3.take(train_size)\n",
    "temp_dataset = dataset3.skip(train_size)\n",
    "val_dataset15k = temp_dataset.take(val_size)\n",
    "test_dataset15k = temp_dataset.skip(val_size)\n",
    "\n",
    "train_identity_losses = []\n",
    "train_age_losses = []\n",
    "val_identity_losses = []\n",
    "val_age_losses = []\n",
    "\n",
    "save_dir=r'/kaggle/working/model_weights2'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "train_dataset15k = train_dataset15k.shuffle(buffer_size=1000)\n",
    "identity_model.load_weights(r'/kaggle/working/model_weights/identity_model_weights14_epoch_60.weights.h5',skip_mismatch=True)\n",
    "age_Estimation_model.load_weights(r'/kaggle/working/model_weights/age_model_weights14_epoch_60.weights.h5',skip_mismatch=True)\n",
    "count=10\n",
    "for epoch in range(10):  \n",
    "    print(f\"Epoch {epoch + 1}/10\")\n",
    "    epoch_identity_loss = []\n",
    "    epoch_age_loss = []\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(train_dataset15k):\n",
    "     \n",
    "\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "        print(identity_inputs.shape)\n",
    "        print(age_inputs.shape)\n",
    "      \n",
    "        history_identity = identity_model.fit(\n",
    "            x=identity_inputs, y=identities,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=10\n",
    "        )\n",
    "\n",
    "        history_age = age_Estimation_model.fit(\n",
    "            x=age_inputs, y=ages,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=10  \n",
    "        )\n",
    "\n",
    "        print(f\"Batch {batch + 1} processed. \"\n",
    "              f\"Identity Loss: {history_identity.history['loss'][0]:.4f}, \"\n",
    "              f\"Age Loss: {history_age.history['loss'][0]:.4f}\")\n",
    "    val_identity_loss = []\n",
    "    val_age_loss = []\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(val_dataset15k):\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "        val_identity_pred = identity_model.predict(identity_inputs)\n",
    "\n",
    "        val_identity_loss.append(tf.keras.losses.sparse_categorical_crossentropy(identities, val_identity_pred))\n",
    "\n",
    "        ages = tf.cast(ages, tf.float32)\n",
    "\n",
    "        val_age_pred = age_Estimation_model.predict(age_inputs)\n",
    "\n",
    "        val_age_pred = tf.cast(tf.argmax(val_age_pred, axis=1), tf.float32)\n",
    "\n",
    "        val_age_loss.append(tf.reduce_mean(tf.abs(ages - val_age_pred)))\n",
    "        \n",
    "    val_identity_losses.append(sum(val_identity_loss) / len(val_identity_loss))\n",
    "    val_age_losses.append(sum(val_age_loss) / len(val_age_loss))\n",
    "    print(\"val identiy:\",sum(val_identity_loss) / len(val_identity_loss))\n",
    "    print(\"val age :\",sum(val_age_loss) / len(val_identity_loss))\n",
    "    if (epoch + 1) % 2 == 1:  \n",
    "        print(f\"Saving weights for epoch {epoch + 1}\")\n",
    "        identity_model.save_weights(os.path.join(save_dir,f'identity_model_weights15_epoch_{count+epoch + 1}.weights.h5'))\n",
    "        age_Estimation_model.save_weights(os.path.join(save_dir,f'age_model_weights15_epoch_{count+epoch + 1}.weights.h5'))\n",
    "\n",
    "    count+=1\n",
    " \n",
    "    tf.keras.backend.clear_session()\n",
    "epochs = range(1, len(train_identity_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_identity_losses, 'bo-', label='Training Identity Loss')\n",
    "plt.plot(epochs, val_identity_losses, 'ro-', label='Validation Identity Loss')\n",
    "plt.title('Identity Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_age_losses, 'bo-', label='Training Age Loss')\n",
    "plt.plot(epochs, val_age_losses, 'ro-', label='Validation Age Loss')\n",
    "plt.title('Age Estimation Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2afd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img_Tensor20K = np.load(r'/kaggle/input/tensors/img_tensor20k.npy')\n",
    "label_tensor = np.load(r'/kaggle/input/tensors/labels.npy')\n",
    "\n",
    "def generator4():\n",
    "    gtrcount = 15000\n",
    "    for i in range(0, 5000):\n",
    "        img = img_Tensor20K[i]\n",
    "        label = label_tensor[gtrcount + i]\n",
    "\n",
    "        inputt = tf.expand_dims(preprocessing(img), axis=0)\n",
    "      \n",
    "        feature_map = base_model(inputt, training=False)\n",
    "\n",
    "        chanoutput = channelattention(feature_map)\n",
    "        spatialoutput = spatialattention(feature_map)\n",
    "        attentionmask1 = chanoutput + spatialoutput\n",
    "        attentionmask2=channelattention(attentionmask1)\n",
    "        attentionmask=spatialattention(attentionmask2)\n",
    "\n",
    "        age_input = attentionmask * feature_map\n",
    "        grl = GradientReversalLayer()\n",
    "    \n",
    "        reversed_mask = grl(attentionmask)\n",
    "    \n",
    "        identity_input = tf.multiply(reversed_mask, feature_map)\n",
    " \n",
    "        identity = label[0]\n",
    "        age = label[1]\n",
    "        gtrcount += 1\n",
    "        yield (identity_input, age_input), (identity, age)\n",
    "\n",
    "dataset4 = tf.data.Dataset.from_generator(\n",
    "    generator4,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(1,7, 7, 2048), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(1, 7, 7, 2048), dtype=tf.float32)),\n",
    "        (tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32))\n",
    "    )\n",
    ")\n",
    "\n",
    "batch_size = 10\n",
    "dataset4 = dataset4.batch(batch_size)\n",
    "\n",
    "dataset4 = dataset4.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "channelattention = ChannelAttention(2048)\n",
    "spatialattention = SpatialAttention()\n",
    "dataset_size = 5000 // 10  \n",
    "\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.15 * dataset_size)  \n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset20k = dataset4.take(train_size)\n",
    "temp_dataset = dataset4.skip(train_size)\n",
    "val_dataset20k = temp_dataset.take(val_size)\n",
    "test_dataset20k = temp_dataset.skip(val_size)\n",
    "\n",
    "\n",
    "train_identity_losses = []\n",
    "train_age_losses = []\n",
    "val_identity_losses = []\n",
    "val_age_losses = []\n",
    "\n",
    "save_dir = r'/kaggle/working/model_weights2'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "identity_model.load_weights(r'/kaggle/working/model_weights3/identity_model_weights16_epoch_25.weights.h5',skip_mismatch=True)\n",
    "age_Estimation_model.load_weights(r'/kaggle/working/model_weights3/age_model_weights16_epoch_25.weights.h5',skip_mismatch=True)\n",
    "train_dataset20k = train_dataset20k.shuffle(buffer_size=1000)\n",
    "count = 1\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}/5\")\n",
    "    epoch_identity_loss = []\n",
    "    epoch_age_loss = []\n",
    "\n",
    "    train_identity_loss = []\n",
    "    train_age_loss = []\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(train_dataset20k):\n",
    "       \n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "        history_identity = identity_model.fit(\n",
    "            x=identity_inputs, y=identities,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=10 \n",
    "        )\n",
    "\n",
    "        history_age = age_Estimation_model.fit(\n",
    "            x=age_inputs, y=ages,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=10  \n",
    "        )\n",
    "\n",
    "        epoch_identity_loss.append(history_identity.history['loss'][0])\n",
    "        epoch_age_loss.append(history_age.history['loss'][0])\n",
    "        print(f\"Batch {batch + 1} processed. \"\n",
    "              f\"Identity Loss: {history_identity.history['loss'][0]:.4f}, \"\n",
    "              f\"Age Loss: {history_age.history['loss'][0]:.4f}\")\n",
    "    train_identity_loss.append(history_identity.history['loss'][0])\n",
    "    train_age_loss.append(history_age.history['loss'][0])\n",
    "\n",
    "    train_dataset20k = train_dataset20k.shuffle(buffer_size=1000)\n",
    " \n",
    "    val_identity_loss = []\n",
    "    val_age_loss = []\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(val_dataset20k):\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "        val_identity_pred = identity_model.predict(identity_inputs)\n",
    "\n",
    "        val_identity_loss.append(tf.keras.losses.sparse_categorical_crossentropy(identities, val_identity_pred))\n",
    "\n",
    "        ages = tf.cast(ages, tf.float32)\n",
    "\n",
    "        val_age_pred = age_Estimation_model.predict(age_inputs)\n",
    "\n",
    "        val_age_pred = tf.cast(tf.argmax(val_age_pred, axis=1), tf.float32)\n",
    "\n",
    "        val_age_loss.append(tf.reduce_mean(tf.abs(ages - val_age_pred)))\n",
    "    \n",
    "    val_identity_losses.append(sum(val_identity_loss) / len(val_identity_loss))\n",
    "    val_age_losses.append(sum(val_age_loss) / len(val_age_loss))\n",
    "    print(\"val id loss:\",sum(val_identity_loss) / len(val_identity_loss))\n",
    "    print(\"val age loss:\",sum(val_age_loss) / len(val_age_loss))\n",
    "    if (epoch + 1) % 2 == 1:  \n",
    "        print(f\"Saving weights for epoch {epoch + 1}\")\n",
    "        identity_model.save_weights(os.path.join(save_dir,f'identity_model_weights25_epoch_{count+epoch + 1}.weights.h5'))\n",
    "        age_Estimation_model.save_weights(os.path.join(save_dir,f'age_model_weights25_epoch_{count+epoch + 1}.weights.h5'))\n",
    "\n",
    "    count+=1\n",
    "   \n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Average Identity Loss: {tf.reduce_mean(val_identity_loss)}\")\n",
    "    print(f\"Average Age Loss: {tf.reduce_mean(val_age_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4dc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_Tensor25K=np.load(r'/kaggle/input/tensors/img_tensor25k.npy')\n",
    "label_tensor=np.load(r'/kaggle/input/tensors/labels.npy')\n",
    "def generator5():\n",
    "        gtrcount=20000\n",
    "        for i in range(0,5000):\n",
    "            img = img_Tensor25K[i]\n",
    "\n",
    "            label = label_tensor[gtrcount+i]\n",
    "\n",
    "\n",
    "            inputt = tf.expand_dims(preprocessing(img), axis=0)\n",
    "       \n",
    "            feature_map = base_model(inputt, training=False)\n",
    "\n",
    "            chanoutput = channelattention(feature_map)\n",
    "            spatialoutput = spatialattention(feature_map)\n",
    "            attentionmask1 = chanoutput + spatialoutput\n",
    "            attentionmask2=channelattention(attentionmask1)\n",
    "            attentionmask=spatialattention(attentionmask2)\n",
    "\n",
    "            age_input = attentionmask * feature_map\n",
    "            grl = GradientReversalLayer()\n",
    "    \n",
    "            reversed_mask = grl(attentionmask)\n",
    "    \n",
    "            identity_input = tf.multiply(reversed_mask, feature_map)\n",
    "\n",
    "            identity = label[0]\n",
    "            age = label[1]\n",
    "            gtrcount+=1\n",
    "            yield (identity_input, age_input), (identity, age)\n",
    "\n",
    "dataset5= tf.data.Dataset.from_generator(\n",
    "    generator5,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(1,7, 7, 2048), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(1, 7, 7, 2048), dtype=tf.float32)),\n",
    "        (tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32))\n",
    "    )\n",
    ")\n",
    "batch_size = 30\n",
    "dataset5 = dataset5.batch(batch_size)\n",
    "\n",
    "dataset5 = dataset5.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "channelattention = ChannelAttention(2048)\n",
    "spatialattention = SpatialAttention()\n",
    "dataset_size = 5000 //30  \n",
    "\n",
    "train_size = int(0.7 * dataset_size) \n",
    "val_size = int(0.15 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_identity_losses = []\n",
    "train_age_losses = []\n",
    "val_identity_losses = []\n",
    "val_age_losses = []\n",
    "\n",
    "train_dataset25k = dataset5.take(train_size)\n",
    "temp_dataset = dataset5.skip(train_size)\n",
    "val_dataset25k = temp_dataset.take(val_size)\n",
    "test_dataset25k = temp_dataset.skip(val_size)\n",
    "\n",
    "\n",
    "\n",
    "save_dir=r'/kaggle/working/model_weights3'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "train_dataset25k = train_dataset25k.shuffle(buffer_size=1000)\n",
    "identity_model.load_weights(r'/kaggle/working/model_weights3/identity_model_weights16_epoch_21.weights.h5',skip_mismatch=True)\n",
    "age_Estimation_model.load_weights(r'/kaggle/working/model_weights3/age_model_weights16_epoch_21.weights.h5',skip_mismatch=True)\n",
    "count=20\n",
    "val_identity_losses=[]\n",
    "val_identity_losses=[]\n",
    "for epoch in range(10):  \n",
    "    print(f\"Epoch {epoch + 1}/10\")\n",
    "    epoch_identity_loss = []\n",
    "    epoch_age_loss = []\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(train_dataset25k):\n",
    "   \n",
    "\n",
    "\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "        history_identity = identity_model.fit(\n",
    "            x=identity_inputs, y=identities,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=30 \n",
    "        )\n",
    "\n",
    "        history_age = age_Estimation_model.fit(\n",
    "            x=age_inputs, y=ages,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=30 \n",
    "        )\n",
    "#\n",
    "        epoch_identity_loss.append(history_identity.history['loss'][0])\n",
    "        epoch_age_loss.append(history_age.history['loss'][0])\n",
    "        print(f\"Batch {batch + 1} processed. \"\n",
    "              f\"Identity Loss: {history_identity.history['loss'][0]:.4f}, \"\n",
    "              f\"Age Loss: {history_age.history['loss'][0]:.4f}\")\n",
    "    train_identity_losses.append(sum(epoch_identity_loss) / len(epoch_identity_loss))\n",
    "    train_age_losses.append(sum(epoch_age_loss) / len(epoch_age_loss))\n",
    "    val_age_loss=[]\n",
    "    val_identity_loss=[]\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(val_dataset25k):\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "\n",
    "        val_identity_pred = identity_model.predict(identity_inputs)\n",
    "\n",
    "        val_identity_loss.append(tf.keras.losses.sparse_categorical_crossentropy(identities, val_identity_pred))\n",
    "\n",
    "        ages = tf.cast(ages, tf.float32)\n",
    "\n",
    "        val_age_pred = age_Estimation_model.predict(age_inputs)\n",
    "\n",
    "        val_age_pred = tf.cast(tf.argmax(val_age_pred, axis=1), tf.float32)\n",
    "\n",
    "        val_age_loss.append(tf.reduce_mean(tf.abs(ages - val_age_pred)))\n",
    "        \n",
    "    val_identity_losses.append(sum(val_identity_loss) / len(val_identity_loss))\n",
    "    val_age_losses.append(sum(val_age_loss) / len(val_age_loss))\n",
    "    print(\"val identiy:\",sum(val_identity_loss) / len(val_identity_loss))\n",
    "    print(\"val age :\",sum(val_age_loss) / len(val_identity_loss))\n",
    "    if (epoch + 1) % 2 == 1:  \n",
    "        print(f\"Saving weights for epoch {epoch + 1}\")\n",
    "        identity_model.save_weights(os.path.join(save_dir,f'identity_model_weights16_epoch_{count+epoch + 1}.weights.h5'))\n",
    "        age_Estimation_model.save_weights(os.path.join(save_dir,f'age_model_weights16_epoch_{count+epoch + 1}.weights.h5'))\n",
    "\n",
    "    count+=1\n",
    " \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "epochs = range(1, len(train_identity_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_identity_losses, 'bo-', label='Training Identity Loss')\n",
    "plt.plot(epochs, val_identity_losses, 'ro-', label='Validation Identity Loss')\n",
    "plt.title('Identity Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_age_losses, 'bo-', label='Training Age Loss')\n",
    "plt.plot(epochs, val_age_losses, 'ro-', label='Validation Age Loss')\n",
    "plt.title('Age Estimation Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29efab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "label_tensor=np.load(r'/kaggle/input/tensors/labels.npy')\n",
    "lable_tensor=label_tensor[25000:30000]\n",
    "print(len(lable_tensor))\n",
    "\n",
    "img_Tensor30K=np.load(r'/kaggle/input/tensors/img_tensor30k.npy')\n",
    "label_tensor=np.load(r'/kaggle/input/tensors/labels.npy')\n",
    "def generator6():\n",
    "        for i in range(0,5000):\n",
    "            img = img_Tensor30K[i]\n",
    "            label = lable_tensor[i]\n",
    "\n",
    "\n",
    "\n",
    "            inputt = tf.expand_dims(preprocessing(img), axis=0)\n",
    "      \n",
    "            feature_map = base_model(inputt, training=False)\n",
    "\n",
    "            chanoutput = channelattention(feature_map)\n",
    "            spatialoutput = spatialattention(feature_map)\n",
    "            attentionmask1 = chanoutput + spatialoutput\n",
    "            attentionmask2=channelattention(attentionmask1)\n",
    "            attentionmask=spatialattention(attentionmask2)\n",
    "\n",
    "\n",
    "            age_input = attentionmask * feature_map\n",
    "            grl = GradientReversalLayer()\n",
    "    \n",
    "            reversed_mask = grl(attentionmask)\n",
    "    \n",
    "            identity_input = tf.multiply(reversed_mask, feature_map)\n",
    "\n",
    "            identity = label[0]\n",
    "            age = label[1]\n",
    "            yield (identity_input, age_input), (identity, age)\n",
    "\n",
    "dataset6= tf.data.Dataset.from_generator(\n",
    "    generator6,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(1,7, 7, 2048), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(1, 7, 7, 2048), dtype=tf.float32)),\n",
    "        (tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32))\n",
    "    )\n",
    ")\n",
    "\n",
    "batch_size = 30\n",
    "dataset6 = dataset6.batch(batch_size)\n",
    "\n",
    "\n",
    "dataset6 = dataset6.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "channelattention = ChannelAttention(2048)\n",
    "spatialattention = SpatialAttention()\n",
    "\n",
    "dataset_size = 5000 //30  \n",
    "\n",
    "train_size = int(0.7 * dataset_size)  \n",
    "val_size = int(0.15 * dataset_size) \n",
    "test_size = dataset_size - train_size - val_size \n",
    "\n",
    "\n",
    "train_dataset30k = dataset6.take(train_size)\n",
    "temp_dataset = dataset6.skip(train_size)\n",
    "val_dataset30k = temp_dataset.take(val_size)\n",
    "test_dataset30k = temp_dataset.skip(val_size)\n",
    "\n",
    "train_identity_losses = []\n",
    "train_age_losses = []\n",
    "val_identity_losses = []\n",
    "val_age_losses = []\n",
    "\n",
    "\n",
    "save_dir=r'/kaggle/working/model_weights'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "train_dataset30k = train_dataset30k.shuffle(buffer_size=1000)\n",
    "identity_model.load_weights(r'/kaggle/working/model_weights3/age_model_weights16_epoch_29.weights.h5',skip_mismatch=True)\n",
    "age_Estimation_model.load_weights(r'/kaggle/working/model_weights3/age_model_weights16_epoch_29.weights.h5',skip_mismatch=True)\n",
    "count=36\n",
    "for epoch in range(5):  \n",
    "    print(f\"Epoch {epoch + 1}/10\")\n",
    "    epoch_identity_loss = []\n",
    "    epoch_age_loss = []\n",
    "\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(train_dataset30k):\n",
    "      \n",
    "\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "        history_identity = identity_model.fit(\n",
    "            x=identity_inputs, y=identities,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=20 \n",
    "        )\n",
    "\n",
    "        history_age = age_Estimation_model.fit(\n",
    "            x=age_inputs, y=ages,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=20 \n",
    "        )\n",
    "\n",
    "        epoch_identity_loss.append(history_identity.history['loss'][0])\n",
    "        epoch_age_loss.append(history_age.history['loss'][0])\n",
    "        print(f\"Batch {batch + 1} processed. \"\n",
    "              f\"Identity Loss: {history_identity.history['loss'][0]:.4f}, \"\n",
    "              f\"Age Loss: {history_age.history['loss'][0]:.4f}\")\n",
    "\n",
    "    train_identity_losses.append(sum(epoch_identity_loss) / len(epoch_identity_loss))\n",
    "    train_age_losses.append(sum(epoch_age_loss) / len(epoch_age_loss))\n",
    "    val_identity_loss = []\n",
    "    val_age_loss = []\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(val_dataset30k):\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "\n",
    "        val_identity_pred = identity_model.predict(identity_inputs)\n",
    "\n",
    "        val_identity_loss.append(tf.keras.losses.sparse_categorical_crossentropy(identities, val_identity_pred))\n",
    "\n",
    "        ages = tf.cast(ages, tf.float32)\n",
    "\n",
    "        val_age_pred = age_Estimation_model.predict(age_inputs)\n",
    "\n",
    "        val_age_pred = tf.cast(tf.argmax(val_age_pred, axis=1), tf.float32)\n",
    "\n",
    "        val_age_loss.append(tf.reduce_mean(tf.abs(ages - val_age_pred)))\n",
    "\n",
    "    val_identity_losses.append(sum(val_identity_loss) / len(val_identity_loss))\n",
    "    val_age_losses.append(sum(val_age_loss) / len(val_age_loss))\n",
    "    if (epoch + 1) % 2 == 1: \n",
    "        print(f\"Saving weights for epoch {epoch + 1}\")\n",
    "        identity_model.save_weights(os.path.join(save_dir,f'identity_model_weights12_epoch_{count+epoch + 1}.weights.h5'))\n",
    "        age_Estimation_model.save_weights(os.path.join(save_dir,f'age_model_weights12_epoch_{count+epoch + 1}.weights.h5'))\n",
    "    \n",
    "    count+=1\n",
    "   \n",
    "    tf.keras.backend.clear_session()\n",
    "epochs = range(1, len(train_identity_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_identity_losses, 'bo-', label='Training Identity Loss')\n",
    "plt.plot(epochs, val_identity_losses, 'ro-', label='Validation Identity Loss')\n",
    "plt.title('Identity Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_age_losses, 'bo-', label='Training Age Loss')\n",
    "plt.plot(epochs, val_age_losses, 'ro-', label='Validation Age Loss')\n",
    "plt.title('Age Estimation Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3418003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff36c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img_Tensor31K=np.load(r'/kaggle/input/tensors/img_tensor31k.npy')\n",
    "label_tensor=np.load(r'/kaggle/input/tensors/labels.npy')\n",
    "labeltensor31=label_tensor[30000:]\n",
    "def generator7():\n",
    "        gtrcount=30000\n",
    "        for i in range(0,1464):\n",
    "            img = img_Tensor31K[i]\n",
    "            label = labeltensor31[i]\n",
    "\n",
    "\n",
    "\n",
    "            inputt = tf.expand_dims(preprocessing(img), axis=0)\n",
    "      \n",
    "            feature_map = base_model(inputt, training=False)\n",
    "\n",
    "            chanoutput = channelattention(feature_map)\n",
    "            spatialoutput = spatialattention(feature_map)\n",
    "            attentionmask1 = chanoutput + spatialoutput\n",
    "            attentionmask2=channelattention(attentionmask1)\n",
    "            attentionmask=spatialattention(attentionmask2)\n",
    "\n",
    "\n",
    "            age_input = attentionmask * feature_map\n",
    "            grl = GradientReversalLayer()\n",
    "    \n",
    "            reversed_mask = grl(attentionmask)\n",
    "    \n",
    "            identity_input = tf.multiply(reversed_mask, feature_map)\n",
    "\n",
    "            identity = label[0]\n",
    "            age = label[1]\n",
    "            gtrcount+=1\n",
    "            yield (identity_input, age_input), (identity, age)\n",
    "\n",
    "dataset7= tf.data.Dataset.from_generator(\n",
    "    generator7,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(1,7, 7, 2048), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(1, 7, 7, 2048), dtype=tf.float32)),\n",
    "        (tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32))\n",
    "    )\n",
    ")\n",
    "\n",
    "batch_size = 20\n",
    "dataset7 = dataset7.batch(batch_size)\n",
    "\n",
    "\n",
    "dataset7 = dataset7.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "channelattention = ChannelAttention(2048)\n",
    "spatialattention = SpatialAttention()\n",
    "\n",
    "\n",
    "dataset_size = 1464 //20 \n",
    "\n",
    "train_size = int(0.7 * dataset_size)  \n",
    "val_size = int(0.15 * dataset_size) \n",
    "test_size = dataset_size - train_size - val_size \n",
    "train_dataset31k = dataset7.take(train_size)\n",
    "temp_dataset = dataset7.skip(train_size)\n",
    "val_dataset31k = temp_dataset.take(val_size)\n",
    "test_dataset31k = temp_dataset.skip(val_size)\n",
    "\n",
    "train_identity_losses = []\n",
    "train_age_losses = []\n",
    "val_identity_losses = []\n",
    "val_age_losses = []\n",
    "\n",
    "save_dir=r'/kaggle/working/model_weights/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "train_dataset31k = train_dataset31k.shuffle(buffer_size=1000)\n",
    "count=38\n",
    "for epoch in range(10):  \n",
    "    print(f\"Epoch {epoch + 1}/10\")\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(train_dataset31k):\n",
    "       \n",
    "        print(identity_inputs.shape)\n",
    "        print(age_inputs.shape)\n",
    "\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "        print(identity_inputs.shape)\n",
    "        print(age_inputs.shape)\n",
    "       \n",
    "        history_identity = identity_model.fit(\n",
    "            x=identity_inputs, y=identities,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=20 \n",
    "        )\n",
    "\n",
    "        history_age = age_Estimation_model.fit(\n",
    "            x=age_inputs, y=ages,\n",
    "            epochs=1, verbose=0,\n",
    "            batch_size=20  \n",
    "        )\n",
    "\n",
    "        print(f\"Batch {batch + 1} processed. \"\n",
    "              f\"Identity Loss: {history_identity.history['loss'][0]:.4f}, \"\n",
    "              f\"Age Loss: {history_age.history['loss'][0]:.4f}\")\n",
    "    val_identity_loss = []\n",
    "    val_age_loss = []\n",
    "    for batch, ((identity_inputs, age_inputs), (identities, ages)) in enumerate(val_dataset31k):\n",
    "        identity_inputs = tf.squeeze(identity_inputs, axis=1)\n",
    "        age_inputs = tf.squeeze(age_inputs, axis=1)\n",
    "\n",
    "        val_identity_pred = identity_model.predict(identity_inputs)\n",
    "\n",
    "        val_identity_loss.append(tf.keras.losses.sparse_categorical_crossentropy(identities, val_identity_pred))\n",
    "\n",
    "        ages = tf.cast(ages, tf.float32)\n",
    "\n",
    "        val_age_pred = age_Estimation_model.predict(age_inputs)\n",
    "\n",
    "        val_age_pred = tf.cast(tf.argmax(val_age_pred, axis=1), tf.float32)\n",
    "\n",
    "        val_age_loss.append(tf.reduce_mean(tf.abs(ages - val_age_pred)))\n",
    "    val_identity_losses.append(sum(val_identity_loss) / len(val_identity_loss))\n",
    "    val_age_losses.append(sum(val_age_loss) / len(val_age_loss))\n",
    "\n",
    "    if (epoch + 1) % 2 == 1:  \n",
    "        print(f\"Saving weights for epoch {epoch + 1}\")\n",
    "        identity_model.save_weights(os.path.join(save_dir,f'identity_model_weights7_epoch_{count+epoch + 1}.weights.h5'))\n",
    "        age_Estimation_model.save_weights(os.path.join(save_dir,f'age_model_weights7_epoch_{count+epoch + 1}.weights.h5'))\n",
    "   \n",
    "    count+=1\n",
    "  \n",
    "    tf.keras.backend.clear_session()\n",
    "epochs = range(1, len(train_identity_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_identity_losses, 'bo-', label='Training Identity Loss')\n",
    "plt.plot(epochs, val_identity_losses, 'ro-', label='Validation Identity Loss')\n",
    "plt.title('Identity Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_age_losses, 'bo-', label='Training Age Loss')\n",
    "plt.plot(epochs, val_age_losses, 'ro-', label='Validation Age Loss')\n",
    "plt.title('Age Estimation Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ec3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7a14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d166a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8f209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d2a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742190ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee8b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542c985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f5f4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee0f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e82744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a9028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117ffda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f57b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe14ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1831ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8a661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce825f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
